{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-10 22:06:58.554138: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-10 22:06:58.689365: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-10 22:06:58.689943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 22:06:59.996953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification ,BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:07:05.564579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-05-10 22:07:05.564613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: DESKTOP-KM5AJMU.home\n",
      "2023-05-10 22:07:05.564619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: DESKTOP-KM5AJMU.home\n",
      "2023-05-10 22:07:05.564712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 530.41.3\n",
      "2023-05-10 22:07:05.564728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.89.2\n",
      "2023-05-10 22:07:05.564733: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 525.89.2 does not match DSO version 530.41.3 -- cannot find working devices in this configuration\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification ,BertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert quantization \n",
    "#! pip install tensorflow-model-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Look at this thing, what even is it?', \"I don't know what you'd call it, but I do know that when I hover this card and it appears, I love it.\", 'Nicely done, Chronicle.', 'Now, someone with a greater understanding of design, please tell us in the comments how so few pixels can be so wonderfully impactful.', \"And while I can't tell you that, I can show you how to recreate it.\", 'Because the reality is that this little fella is nothing more than a linear gradient.', \"But just as important as what it is, is how it's perceived by the user.\", \"Upon closer inspection, we'll find that this gradient is just one pixel wide and a few dozen pixels tall.\", 'It starts transparent, fades to a color, and then back to transparent.', \"If we temporarily make it wider, we'll find that it appears no different than any other gradient.\", \"But as we reduce the width, we'll notice that eventually, because our eyes are not able to perceive a single pixel very well, it begins to look more like a fragmented shard.\", \"So if we hit up our friendly neighborhood code pen, throw together a basic card with some text inside, and then save it as private, well actually that's the super handy pro feature that I use all the time, so my pen doesn't show up in my profile before I finish a video.\", 'We can then slap our gradient into a pseudo element, position it absolutely along the left side, and down a little ways from the top, not forgetting to set content to an empty string so our after element shows up, and to set position relative on the card so the left and top values correspond to it rather than the whole document.', 'After setting a transition time on the top and opacity properties, we can shift those values on hover so our shard fades into view.', 'Remember to use this one wisely, friends.', 'Gradients are powerful tools, and with great power comes gradient responsibility.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/Djoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##text splitting\n",
    "nltk.download('punkt')\n",
    "paragraph = \" Look at this thing, what even is it? I don't know what you'd call it, but I do know that when I hover this card and it appears, I love it. Nicely done, Chronicle. Now, someone with a greater understanding of design, please tell us in the comments how so few pixels can be so wonderfully impactful. And while I can't tell you that, I can show you how to recreate it. Because the reality is that this little fella is nothing more than a linear gradient. But just as important as what it is, is how it's perceived by the user. Upon closer inspection, we'll find that this gradient is just one pixel wide and a few dozen pixels tall. It starts transparent, fades to a color, and then back to transparent. If we temporarily make it wider, we'll find that it appears no different than any other gradient. But as we reduce the width, we'll notice that eventually, because our eyes are not able to perceive a single pixel very well, it begins to look more like a fragmented shard. So if we hit up our friendly neighborhood code pen, throw together a basic card with some text inside, and then save it as private, well actually that's the super handy pro feature that I use all the time, so my pen doesn't show up in my profile before I finish a video. We can then slap our gradient into a pseudo element, position it absolutely along the left side, and down a little ways from the top, not forgetting to set content to an empty string so our after element shows up, and to set position relative on the card so the left and top values correspond to it rather than the whole document. After setting a transition time on the top and opacity properties, we can shift those values on hover so our shard fades into view. Remember to use this one wisely, friends. Gradients are powerful tools, and with great power comes gradient responsibility.\"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=URL,\n",
    "                                  untar=True,\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shutil module offers a number of high-level \n",
    "# operations on files and collections of files.\n",
    "import os\n",
    "import shutil\n",
    "# Create main directory path (\"/aclImdb\")\n",
    "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "# Create sub directory path (\"/aclImdb/train\")\n",
    "train_dir = os.path.join(main_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./aclImdb\n",
      "./aclImdb/train\n"
     ]
    }
   ],
   "source": [
    "print(main_dir)\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unsup folder since this is a supervised learning task\n",
    "remove_dir = os.path.join(train_dir, 'unsup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./aclImdb/train/unsup'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'unsupBow.feat', 'labeledBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(remove_dir)\n",
    "# View the final train folder\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:08:37.552159: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [20000]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-10 22:08:37.552438: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [20000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian director Vincenzo Natali took the art...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I gave this film 10 not because it is a superb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being somewhat jaded about the movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For a long time, 'The Menagerie' was my favori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A truly frightening film. Feels as if it were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  Canadian director Vincenzo Natali took the art...            1\n",
       "1  I gave this film 10 not because it is a superb...            1\n",
       "2  I admit to being somewhat jaded about the movi...            1\n",
       "3  For a long time, 'The Menagerie' was my favori...            1\n",
       "4  A truly frightening film. Feels as if it were ...            0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a train dataframe\n",
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy()\n",
    "  train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[0:200,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:08:43.411380: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [5000]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-10 22:08:43.411667: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [5000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't believe that so much talent can be was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie blows - let's get that straight rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm only rating this film as a 3 out of pity b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something surprised me about this movie - it w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  I can't believe that so much talent can be was...            0\n",
       "1  This movie blows - let's get that straight rig...            0\n",
       "2  The saddest thing about this \"tribute\" is that...            0\n",
       "3  I'm only rating this film as a 3 out of pity b...            0\n",
       "4  Something surprised me about this movie - it w...            1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a test dataframe \n",
    "for j in test.take(1):\n",
    "  test_feat = j[0].numpy()\n",
    "  test_lab = j[1].numpy()\n",
    "\n",
    "\"\"\"\n",
    "After executing these lines of code, the test_feat and test_lab variables will contain the\n",
    " input features and labels of the first batch of data from the test dataset, respectively, as numpy arrays.\n",
    "\"\"\"\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test['DATA_COLUMN'] = test['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[0:20,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputExample(guid=None, text_a='Hello, world', text_b=None, label=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputExample(guid=None,\n",
    "             text_a = \"Hello, world\",\n",
    "             text_b = None,\n",
    "             label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'DATA_COLUMN', \n",
    "                                                                           'LABEL_COLUMN')\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Djoo/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:09:12.627145: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-10 22:09:12.627525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14/Unknown - 161s 9s/step - loss: 0.6393 - accuracy: 0.6125 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 22:11:54.066178: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 167s 10s/step - loss: 0.6393 - accuracy: 0.6125 - val_loss: 0.6260 - val_accuracy: 0.5500\n",
      "Epoch 2/2\n",
      "14/14 [==============================] - 131s 9s/step - loss: 0.3908 - accuracy: 0.8875 - val_loss: 0.5275 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf28200c10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\n",
    "                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def get_final_decision(sentences):\n",
    "    tf_batch = tokenizer(sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    labels = ['Negative','Positive']\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    unique_vals = np.arange(np.max(label) + 1)\n",
    "    counts = np.bincount(label, minlength=len(unique_vals))\n",
    "    if counts[1] > counts[0] :\n",
    "        return 'Positive'\n",
    "    elif  counts[1] < counts[0] :\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutre'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_decision(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
